{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Challenge\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the Text Classification Challenge! In this task, you will develop a machine learning model to classify IMDb movie reviews into positive or negative sentiments. The challenge is designed to help you demonstrate your skills in natural language processing (NLP) and your ability to work with state-of-the-art transformer models.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The task is to build a text classification model that accurately predicts whether a given movie review expresses a positive or negative sentiment. Sentiment analysis is a critical task in NLP with applications in marketing, customer feedback, social media monitoring, and more. Accurately classifying sentiments can provide valuable insights into customer opinions and help businesses make data-driven decisions.\n",
    "\n",
    "### Why This Task is Important\n",
    "\n",
    "Understanding customer sentiment through text data is crucial for businesses and organizations to respond effectively to customer needs and preferences. By automating the sentiment analysis process, companies can efficiently analyze vast amounts of data, identify trends, and make informed strategic decisions. For this challenge, we will use the IMDb dataset, a widely-used benchmark in sentiment analysis, to train and evaluate our model.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The dataset used for this challenge is the IMDb movie reviews dataset, which contains 50,000 reviews labeled as either positive or negative. This dataset is balanced, with an equal number of positive and negative reviews, making it ideal for training and evaluating sentiment analysis models.\n",
    "\n",
    "- **Columns:**\n",
    "  - `review`: The text of the movie review.\n",
    "  - `sentiment`: The sentiment label (`positive` or `negative`).\n",
    "\n",
    "The IMDb dataset provides a real-world scenario where understanding sentiment can offer insights into public opinion about movies, directors, and actors, as well as broader trends in the entertainment industry.\n",
    "\n",
    "## Approach\n",
    "\n",
    "Transformers have revolutionized NLP by allowing models to consider the context of a word based on surrounding words, enabling better understanding and performance on various tasks, including sentiment analysis. Their ability to transfer learning from massive datasets and adapt to specific tasks makes them highly effective for text classification.\n",
    "\n",
    "## Your Task\n",
    "\n",
    "You are required to implement a transformer-based model for sentiment classification on the IMDb dataset. Follow the steps below to complete the challenge:\n",
    "\n",
    "1. **Data Exploration and Preprocessing:**\n",
    "   - Load the dataset and perform exploratory data analysis (EDA) to understand its structure.\n",
    "   - Preprocess the data by cleaning text, encoding labels, and splitting into training and test sets.\n",
    "\n",
    "2. **Model Implementation:**\n",
    "   - Implement a transformer-based model for sentiment classification. You should consider writing Transformer blocks from scratch.\n",
    "   - Implement data loaders and training loops using a deep learning framework like PyTorch or TensorFlow.\n",
    "\n",
    "3. **Training and Evaluation:**\n",
    "   - Train your model and optimize hyperparameters for the best performance.\n",
    "   - Evaluate the model using appropriate metrics.\n",
    "\n",
    "4. **Documentation:**\n",
    "   - Document your approach, experiments, and results.\n",
    "   - Discuss any challenges faced and propose potential improvements.\n",
    "\n",
    "5. **Prediction and Inference:**\n",
    "    - Implement a function that takes a movie review as input and predicts the sentiment (positive or negative).\n",
    "    - Test the function with custom reviews and display the predicted sentiment.\n",
    "\n",
    "6. **Model Deployment:**\n",
    "    - Save the trained model and any other necessary files.\n",
    "    - Prepare the model for deployment (e.g., using Flask or FastAPI).\n",
    "    - Prepare a basic front-end interface for the deployed model.\n",
    "\n",
    "7. **Submission:**\n",
    "    - Create a GitHub repository for your code.\n",
    "    - Write a detailed README.md file with instructions on how to train, evaluate, and use the model.\n",
    "    - Include a summary of your approach and the results in the README file.\n",
    "    - Your code should be well-documented and reproducible.\n",
    "    - Your repository should include a notebook showcasing the complete process, including data loading, preprocessing, model implementation, training, and evaluation.\n",
    "    - Apart from the notebook, you should also have all the codes in .py files so that it can be easily integrated with the API.\n",
    "    - You submission should also include a python script for the API.\n",
    "    - Your submission should also include a basic front-end for the deployed model.\n",
    "    - Submit the GitHub repository link.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To get started, follow the structure provided in this notebook, complete each step, and explore additional techniques to enhance your model's performance. Make sure to document your findings and prepare a comprehensive report on your work.\n",
    "\n",
    "Good luck, and welcome to RealAI!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Preprocessing\n",
    "\n",
    "Let's start by loading the dataset and performing some exploratory data analysis (EDA) to understand its structure and characteristics.\n",
    "You can download the dataset from the following link: https://drive.google.com/file/d/1aU7Vv7jgodZ0YFOLY7kmSjrPcDDwtRfU/view?usp=sharing\n",
    "\n",
    "You should provide all the necessary reasoning and code to support your findings.\n",
    "\n",
    "Finally, you should apply the required preprocessing steps to prepare the data for training the sentiment classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/temirlanzarymkanov/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 40000\n",
      "Test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, GlobalAveragePooling1D, Dropout, Dense, Layer\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Data Preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'<br />', ' ', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    text = re.sub('\\[[^]]*\\]', '', text) # Remove square brackets and its contents\n",
    "    return text\n",
    "\n",
    "stop=set(stopwords.words('english'))\n",
    "tokenizer=ToktokTokenizer()\n",
    "stopword_list=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "# Load the dataset\n",
    "file_path = 'IMDB Dataset.csv'  # Update this path if necessary\n",
    "df = pd.read_csv(file_path)\n",
    "# Preprocess the text data\n",
    "df['review'] = df['review'].apply(clean_text)\n",
    "df['review']=df['review'].apply(remove_stopwords)\n",
    "# Encode output labels\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Define maximum number of words and sequence length\n",
    "MAX_NUM_WORDS = 20000  # Vocabulary size\n",
    "MAX_SEQUENCE_LENGTH = 200  # Max length for each review\n",
    "\n",
    "# Initialize and fit the tokenizer on the text data\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(df['review'].values)\n",
    "\n",
    "# Convert text data to padded sequences\n",
    "sequences = tokenizer.texts_to_sequences(df['review'].values)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, df['sentiment'].values, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_val.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the tokenizer for use in FastAPI\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation\n",
    "\n",
    "You are required to implement a transformer-based model for sentiment classification from scratch. You can use libraries like PyTorch or TensorFlow to implement the model architecture and training process.\n",
    "\n",
    "You should include the architecture figure of the proposed model and provide a detailed explanation of why you chose this architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, maxlen, embed_dim, **kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.maxlen = maxlen\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        position_indices = tf.range(self.maxlen, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, self.embed_dim, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / self.embed_dim))\n",
    "        \n",
    "        # Create positional encoding matrix using TensorFlow operations\n",
    "        sinusoids = tf.expand_dims(position_indices * div_term, -1)\n",
    "        pos_enc = tf.concat([tf.sin(sinusoids), tf.cos(sinusoids)], axis=-1)\n",
    "        pos_enc = tf.reshape(pos_enc, [1, self.maxlen, self.embed_dim])\n",
    "        \n",
    "        return inputs + pos_enc\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncoding, self).get_config()\n",
    "        config.update({\n",
    "            \"maxlen\": self.maxlen,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TransformerBlock, self).get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = Dense(embed_dim)\n",
    "        self.key_dense = Dense(embed_dim)\n",
    "        self.value_dense = Dense(embed_dim)\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        attention, _ = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_encoding             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncoding</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">297,344</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m2,560,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_encoding             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mPositionalEncoding\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m297,344\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,857,473</span> (10.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,857,473\u001b[0m (10.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,857,473</span> (10.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,857,473\u001b[0m (10.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, ff_dim):\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n",
    "    \n",
    "    # Positional encoding\n",
    "    positional_encoding = PositionalEncoding(maxlen, embed_dim)(embedding_layer)\n",
    "    \n",
    "    # Transformer block\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)(positional_encoding, training=True)\n",
    "    \n",
    "    # Pooling and output layers\n",
    "    pooling_layer = GlobalAveragePooling1D()(transformer_block)\n",
    "    dropout_layer = Dropout(0.1)(pooling_layer)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(dropout_layer)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Model parameters\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "\n",
    "# Build the model\n",
    "model = build_transformer_model(MAX_SEQUENCE_LENGTH, MAX_NUM_WORDS, embed_dim, num_heads, ff_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation\n",
    "\n",
    "Train your sentiment classification model on the preprocessed data. You should experiment with different hyperparameters and training configurations to achieve the best performance.\n",
    "\n",
    "Evaluate your model using appropriate metrics and provide an analysis of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 128ms/step - accuracy: 0.6992 - loss: 0.5176 - val_accuracy: 0.8855 - val_loss: 0.2693\n",
      "Epoch 2/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 125ms/step - accuracy: 0.9262 - loss: 0.1963 - val_accuracy: 0.8896 - val_loss: 0.2695\n",
      "Epoch 3/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 124ms/step - accuracy: 0.9550 - loss: 0.1300 - val_accuracy: 0.8682 - val_loss: 0.3831\n",
      "Epoch 4/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 128ms/step - accuracy: 0.9701 - loss: 0.0912 - val_accuracy: 0.8822 - val_loss: 0.3839\n",
      "Epoch 5/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 127ms/step - accuracy: 0.9838 - loss: 0.0540 - val_accuracy: 0.8764 - val_loss: 0.4469\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 47ms/step - accuracy: 0.8777 - loss: 0.4354\n",
      "Validation Accuracy: 87.64%\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_val, y_val))\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Inference\n",
    "\n",
    "Implement a function that takes a movie review as input and predicts the sentiment (positive or negative). Test the function with custom reviews and display the predicted sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Review: I absolutely loved this movie! The performances were outstanding and the story was captivating.\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Review: The film was terrible. The plot made no sense and the acting was worse.\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Review: An average movie. It had its moments but could have been better.\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Review: A masterpiece! The director has outdone himself with this one.\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Review: I was very disappointed with the storyline. It was too predictable.\n",
      "Predicted Sentiment: negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CODE HERE\n",
    "def preprocess_review(review, tokenizer, max_sequence_length):\n",
    "    sequence = tokenizer.texts_to_sequences([review])\n",
    "    \n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length)\n",
    "    \n",
    "    return padded_sequence\n",
    "\n",
    "def predict_sentiment(review, model, tokenizer, max_sequence_length):\n",
    "    processed_review = preprocess_review(review, tokenizer, max_sequence_length)\n",
    "    \n",
    "    prediction = model.predict(processed_review)\n",
    "    \n",
    "    sentiment = \"positive\" if prediction[0] > 0.5 else \"negative\"\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "custom_reviews = [\n",
    "    \"I absolutely loved this movie! The performances were outstanding and the story was captivating.\",\n",
    "    \"The film was terrible. The plot made no sense and the acting was worse.\",\n",
    "    \"An average movie. It had its moments but could have been better.\",\n",
    "    \"A masterpiece! The director has outdone himself with this one.\",\n",
    "    \"I was very disappointed with the storyline. It was too predictable.\"\n",
    "]\n",
    "\n",
    "# Test the function with custom reviews\n",
    "for review in custom_reviews:\n",
    "    sentiment = predict_sentiment(review, model, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "    print(f\"Review: {review}\\nPredicted Sentiment: {sentiment}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Save the trained model and any other necessary files. Prepare the model for deployment using Flask or FastAPI. Make a python script for the API. Also, include a basic front-end for the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentiment_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "You need to create a GitHub repository and submit the link to the repository containing the complete code, documentation, and any other necessary files.\n",
    "\n",
    "The repository should include:\n",
    "- A README file with detailed instructions on how to train, evaluate, and use the model.\n",
    "- A notebook showcasing the complete process, including data loading, preprocessing, model implementation, training, and evaluation.\n",
    "- Python scripts for the training, evaluation, and inference functions.\n",
    "- A python script for the API.\n",
    "- Front-end code for the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
